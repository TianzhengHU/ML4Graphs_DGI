{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c32d24-ccd3-4a73-8609-72c92d5847b0",
   "metadata": {},
   "source": [
    "# Machine Learning for Graphs - Tutorial B: The Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33975479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nbconvert[webpdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb6f7c6-8d41-4cc9-835b-4bd0180be7f6",
   "metadata": {},
   "source": [
    "Fill in your names and group number here:\n",
    "\n",
    "**NAME STUDENT A :** Tianzheng Hu (2760270)\n",
    "\n",
    "**NAME STUDENT B :** Lijing Luo (2794795)\n",
    "\n",
    "**GROUP NUMBER :** 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eaa3c8-1ef9-4a70-b834-3aaf03e1051d",
   "metadata": {},
   "source": [
    "Implementing a machine learning experiment with graph data is an important skill that you will learn as part of this course. This hands-on tutorial will help you develop this skill, as well as help you familiarize yourself with many of the steps and techniques that you will likely need to use for your final project.\n",
    "\n",
    "Representation learning is the task of learning sensible representations for your samples given some downstream task. On graphs, representation learning is commonly used to learn vector representations of the nodes. These node representations are often called *embeddings vectors* or just *embeddings*. Graph Neural Networks (GNN) are ideal for learning node embeddings, since the identity of a node is a function of its neighbourhood (up to depth *d*) and since GNNs learn internal node representations by applying an aggregation operator on exactly this neighbourhood. Different models with various choices of aggregation operator have been introduced over the past couple of years, with the *convolutional* and *attention* operators being the more popular choices.\n",
    "\n",
    "For this tutorial, you are asked to implement the original *Graph Convolutional Network* (GCN) and to replicate some of the classification experiments from the [paper](https://arxiv.org/abs/1609.02907) that introduced it [1]. To help you on your way, we have already prepared this Python Notebook.\n",
    "\n",
    "You are asked to team up with another student and to work together on this tutorial. Please register your team by creating a new group and by adding both members.\n",
    "\n",
    "    [1] Kipf, T. N., & Welling, M. Semi-supervised Classification With Graph Convolutional Networks (2017).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293cfcff-c494-4866-b042-2d575793e833",
   "metadata": {},
   "source": [
    "## NumPy and PyTorch\n",
    "\n",
    "In this course we will make use of the [NumPy](https://numpy.org) package for working with vector data, and the [PyTorch](https://pytorch.org) machine learning package. Both of these are probably already installed in your environment as part of the first tutorial (Numpy as a dependency of PyTorch) but if this is not the case then running the following cell will install these packages for you.\n",
    "\n",
    "**Run the cell below to install the NumPy and PyTorch packages in your Python environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bb3657a-3cf8-4bbb-9290-1f7d3df6867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057c299-0ef8-4655-9184-5f2f71045199",
   "metadata": {},
   "source": [
    "**Run the cells below to import the necessay packages and to set a manual seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dcb06c7-0b58-439b-83a5-21293e6bb4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7db8c53c10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed = 42  # for reproducability\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ddba6",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In the previous tutorial we used the *RDFlib* package to import the dataset. That the dataset was encoded using an open standard made this possible. This is not always the case, however: it is very common to come accross graph datasets that use an arbitrary encoding. In the case of the *Cora* dataset loaded below, the graph has been stored in two parts: the first a set of integer-encoded edges $[i, j]$, with $i$ and $j$ the indices of the nodes, and the second as a set of *n*-hot encoded node representations. Being a citation graph, the edges convey who cites who, whereas the node vectors $e_i$ represent a sparse bag-of-words with vocabulary $\\Omega$ for which holds that $e_i[j] = 1$ if word $\\Omega[j]$ occurs in the document and $0$ otherwise.\n",
    "\n",
    "To import the Cora dataset we first process the raw files using NumPy and cast the generated arrays to the correct datatypes. Next, we generate a node-to-integer map and reindex the edges to ensure that their node identifiers match those of the nodes.\n",
    "\n",
    "**Run the following cells to import and process the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a1481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "path = './data/'\n",
    "\n",
    "data = np.genfromtxt(path + \"cora.content\", dtype = str)\n",
    "edges = np.genfromtxt(path + \"cora.cites\", dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e37c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these have the same order\n",
    "features = data[:, 1:-1].astype(int)\n",
    "labels = data[:, -1]\n",
    "nodes = data[:, 0].astype(int)\n",
    "\n",
    "n2i = {n:i for i,n in enumerate(nodes)}\n",
    "edges_reindexed = np.array([[n2i[source], n2i[target]] for source, target in edges])\n",
    "\n",
    "num_nodes = len(nodes)\n",
    "num_edges = len(edges)\n",
    "num_features = len(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f165fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  31336, 1061127, 1106406, ..., 1128978,  117328,   24043]),\n",
       " (2708,),\n",
       " array([[     35,    1033],\n",
       "        [     35,  103482],\n",
       "        [     35,  103515],\n",
       "        ...,\n",
       "        [ 853118, 1140289],\n",
       "        [ 853155,  853118],\n",
       "        [ 954315, 1155073]]),\n",
       " 5429,\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]),\n",
       " 2708,\n",
       " (5429, 2),\n",
       " array(['Neural_Networks', 'Rule_Learning', 'Reinforcement_Learning', ...,\n",
       "        'Genetic_Algorithms', 'Case_Based', 'Neural_Networks'],\n",
       "       dtype='<U22'),\n",
       " 2708)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes, nodes.shape, edges, len(edges), features, len(features), (edges_reindexed).shape, labels, len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83bc818a-8814-4e55-8c1d-633dc643a7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 2708\n",
      "Number of edges: 5429\n",
      "Number of features: 1433\n",
      "\n",
      "Node ID: 31336\n",
      "Node features: [0 0 0 ... 0 0 0]\n",
      "Node label: Neural_Networks\n",
      "\n",
      "Node ID: 1061127\n",
      "Node features: [0 0 0 ... 0 0 0]\n",
      "Node label: Rule_Learning\n",
      "\n",
      "Node ID: 1106406\n",
      "Node features: [0 0 0 ... 0 0 0]\n",
      "Node label: Reinforcement_Learning\n",
      "\n",
      "Node ID: 13195\n",
      "Node features: [0 0 0 ... 0 0 0]\n",
      "Node label: Reinforcement_Learning\n",
      "\n",
      "Node ID: 37879\n",
      "Node features: [0 0 0 ... 0 0 0]\n",
      "Node label: Probabilistic_Methods\n",
      "\n",
      "Edges: \n",
      "[[ 163  402]\n",
      " [ 163  659]\n",
      " [ 163 1696]\n",
      " [ 163 2295]\n",
      " [ 163 1274]]\n"
     ]
    }
   ],
   "source": [
    "# inspect the data\n",
    "print(f\"Number of nodes: {num_nodes}\")\n",
    "print(f\"Number of edges: {num_edges}\")\n",
    "print(f\"Number of features: {num_features}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Node ID: {nodes[i]}\")\n",
    "    print(f\"Node features: {features[i]}\")\n",
    "    print(f\"Node label: {labels[i]}\\n\")\n",
    "    \n",
    "print(f\"Edges: \\n{edges_reindexed[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d06ddb",
   "metadata": {},
   "source": [
    "## Task 1: Vectorizing the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925195e-b8e5-4429-9768-564fe63f8a18",
   "metadata": {},
   "source": [
    "Since graph neural networks aggregate the information from the neighbourhoods of nodes, they need to know which nodes are adjacent to which other nodes. Because the information from those neighbours must also be aggregated from *their* neighbourhoods, these models thus need a relatively large amount of information about the structure of a graph. This information comes in the form of an *adjacency matrix* $A$, such that $A[i,j] = 1$ if there exists a link between nodes $i$ and $j$, and $0$ otherwise.\n",
    "\n",
    "Of course, the adjacency matrix only tells the model which nodes to aggregate. To also know *what* to aggregate, we need another matrix which uniquely identifies each node. This matrix is often called the *node feature matrix* $X$. If our nodes comes with one or more attributes, or *features*, then we can fill up this matrix with the corresponding values. This is commonly done with *multimodal learning*. More often, however, it is easier to just ignore the node features (if any), and to let $X$ equal the identity matrix $I$ such that $X[i,j] = 1$ iff $i = j$ and $0$ otherwise.\n",
    "\n",
    "Finally, since the downstream task is *node classification*, we need a vector representation, the *target vector* $y$, for the class labels that are used to compute the loss and accuracy scores. Since we need to calculate the gradients during this step, we need a numerical encoding for the labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a66ea26",
   "metadata": {},
   "source": [
    "### Task 1a: Creating a feature matrix\n",
    "\n",
    "Write a procedure to generate a node feature matrix that maps each node to its respective feature vector. The result should be a *sparse* float tensor `X`, such that `X[i]` refers to the feature vector of node `i`. Since the Cora dataset comes with integer-encoded node features (the bag-of-words) there is no need to generate an indentity matrix. Remember that the whole set of features is stored in variable `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b8aac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(features).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec24a25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 1433])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a77ca6",
   "metadata": {},
   "source": [
    "Run the following code to check your feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f93befc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check your feature matrix\n",
    "X.to_dense()[:10,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08678179",
   "metadata": {},
   "source": [
    "### Task 1b: Create an adjacency matrix\n",
    "\n",
    "Write a procedure to generate the adjacency matrix for the Cora graph. The result should be a *sparse* float tensor `A`, such that `A[i,j]` equals `1` if there exists an edge between nodes `i` and `j`, and `0` otherwise. Be aware that the GCN requires all nodes to have a reflexive edge (loops) which ensures that the nodes remember their previous state when updating.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "380be854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import identity\n",
    "\n",
    "adjacency_matrix= np.zeros((num_nodes,num_nodes), dtype = int)\n",
    "identity_matrix = identity(num_nodes)\n",
    "\n",
    "for i in range(len(edges_reindexed)):\n",
    "    row = edges_reindexed[i][0]\n",
    "    col = edges_reindexed[i][1]\n",
    "    adjacency_matrix[row][col] = 1\n",
    "    \n",
    "\n",
    "# res = (adjacency_matrix==adjacency_matrix.T).all()\n",
    "\n",
    "adjacency_matrix = adjacency_matrix + identity_matrix\n",
    "A = torch.from_numpy(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3497674",
   "metadata": {},
   "source": [
    "Run the following code to check your adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b47de91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of connections, 8137, must equal the number of edges, 5429, plus the number of nodes, 2708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check your adjacency matrix by using the sum as proxy\n",
    "print(f\"The number of connections, {int(A.sum())}, must equal the number of edges, {num_edges},\" \n",
    "      f\" plus the number of nodes, {num_nodes}\")\n",
    "A.to_dense()[:10,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96879c15",
   "metadata": {},
   "source": [
    "### Task 1c: Create the target vector\n",
    "\n",
    "Write a procedure to generate the target vector with integer-encoded class labels. The result should be a `long` vector `y_true`, such that `y_true[i]` holds the target label of node `i`. Note that, with PyTorch, different loss functions require differently formatted target vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8d9a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = {ids: i for i, ids in enumerate(set(labels))}\n",
    "y_true = [encoded_labels[label] for label in labels]\n",
    "\n",
    "num_labels = np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d09f3e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e6b00",
   "metadata": {},
   "source": [
    "Run the following code to check your target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "720aa91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique labels: ['Case_Based' 'Genetic_Algorithms' 'Neural_Networks'\n",
      " 'Probabilistic_Methods' 'Reinforcement_Learning' 'Rule_Learning' 'Theory']\n",
      "\n",
      "y: [5, 0, 3, 3, 4, 4, 1, 5, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "print(f'number of unique labels: {num_labels}\\n')\n",
    "\n",
    "print(f'y: {y_true[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ff568",
   "metadata": {},
   "source": [
    "## Task 2: Partition the dataset\n",
    "\n",
    "To properly perform our experiments we first need to partition our data into a _train_ and _test_ split. These splits are used to train and test our model, respectively, and must be disjoint to avoid information leakage. Ideally, we would als create a _validation_ split to use for model selection and/or hyperparameter optimization, but we dispense with that for now.\n",
    "\n",
    "Create a procedure to create a train and test split with a ratio of 4 to 1. The result should be two vectors, `train_idx` and `test_idx`, that contain indices that point to the actual data (a _mask_) that are randomly drawn from the set of all indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a09368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  ...],\n",
       " [True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  ...])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train = num_nodes // 4\n",
    "num_test = num_nodes - num_train\n",
    "\n",
    "# use mask\n",
    "# Generate a mask vector with 1/4 numbers as 1 and others as 0\n",
    "train_mask_vector = np.ones(num_nodes)\n",
    "train_mask_vector[:num_train] = 0\n",
    "np.random.shuffle(train_mask_vector)\n",
    "\n",
    "# Generate another vector in the opposite way\n",
    "test_mask_vector = 1 - train_mask_vector\n",
    "\n",
    "train_mask_vector[:10],test_mask_vector[:10]\n",
    "\n",
    "train_idx = train_mask_vector\n",
    "test_idx = test_mask_vector\n",
    "\n",
    "train_idx, test_idx\n",
    "\n",
    "train_mask = [value == 1 for value in train_idx]\n",
    "test_mask = [value == 1 for value in test_idx]\n",
    "\n",
    "train_mask, test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6400fe68",
   "metadata": {},
   "source": [
    "Run the following code to check your partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c5bde0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples: 677\n",
      "number of testing samples: 2031\n",
      "\n",
      "train indices:\n",
      "[0. 0. 0. 1. 0.]\n",
      "\n",
      "test indices:\n",
      "[1. 1. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of training samples: {num_train}\")\n",
    "print(f\"number of testing samples: {num_test}\")\n",
    "\n",
    "print(f\"\\ntrain indices:\\n{train_idx[:5]}\")\n",
    "print(f\"\\ntest indices:\\n{test_idx[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b03bdd-2e7e-4abd-9e59-1c93fc1f8665",
   "metadata": {},
   "source": [
    "## The Graph Convolutional\n",
    "\n",
    "The *Graph Convolutional Network* (GCN) is arguably the first major breakthrough in GNN development. Developed in 2017, the GCN introduces the idea of the *spectral graph convolution*, which, analogues to its visual counterpart, aggregates the information surrounding an object. In the case of *Convolutional Neural Networks* (CNN), these objects are pixels, whereas with the GCN these are nodes. This comparison becomes evident when you consider images as regular (grid-shaped) graphs with pixel as nodes.\n",
    "\n",
    "The GCN is defined as a network with one or more *Graph Convolution* layers. Each of these layers applies the convolution operator to its input, and is defined as \n",
    "\n",
    "$$ H^{l+1} = \\sigma(\\tilde{D}^{- \\frac{1}{2}} \\tilde{A} \\tilde{D}^{- \\frac{1}{2}} H^l W^l) $$\n",
    "\n",
    "where $\\tilde{A}$ is the adjacency matrix with reflexive edges, $\\tilde{D}$ the degree matrix derived from $\\tilde{A}$, $H^l$ the internal node representations of layer $l$, $W^l$ the weight matrix of layer $l$, and $\\sigma$ a nonlinearity like $ReLU$. Note that the initial node representation matrix $H^0 = X$.\n",
    "\n",
    "In the experiments that we are reproducing the GCN is used for the task of node classification. For this purpose, the GCN is given two graph convolution layers, but with the nonlinearity of the last layer replaced by a softmax function:\n",
    "\n",
    "$$ y = softmax(\\hat{A}~\\sigma(\\hat{A} X W^0)~W^1) $$\n",
    "\n",
    "with $\\hat{A} = \\tilde{D}^{- \\frac{1}{2}} \\tilde{A} \\tilde{D}^{- \\frac{1}{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281a4d5d",
   "metadata": {},
   "source": [
    "### Task 3a: Implement the Graph Convolution\n",
    "\n",
    "Implement the graph convolution layer as a subclass of PyTorch `nn.Module`. Concretely, you must implement the `__init__` and `forward` functions. Ensure that the computation supports sparse tensors, and that the input and output dimensions can be set on initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b50a9a8-2903-494b-8c74-7922fde18ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import diags\n",
    "from torch.nn import Parameter\n",
    "def get_A_hat_torch(A):\n",
    "    A_tilde = coo_matrix(A, dtype=float)\n",
    "    degrees = A_tilde.sum(axis=1).flatten().A\n",
    "    Diag_matrix = diags(degrees, list(range(len(degrees))), dtype=float)\n",
    "    A_hat = (Diag_matrix.power(-0.5) @ A_tilde @ Diag_matrix.power(-0.5)).tocoo()\n",
    "\n",
    "\n",
    "    # A as sparse PyTorch tensor\n",
    "    indices = np.vstack((A_hat.row, A_hat.col))\n",
    "    A_hat_torch = torch.sparse_coo_tensor(indices, A_hat.data, dtype=torch.float)\n",
    "    return A_hat_torch\n",
    "\n",
    "\n",
    "class GraphConvolutionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Graph Convolution Layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # your code here\n",
    "        # Define learnable weight matrix\n",
    "        self.W = Parameter(torch.Tensor(in_features, out_features))\n",
    "        torch.nn.init.kaiming_uniform_(self.W)  # Xavier/Glorot initialization\n",
    "        \n",
    "    def forward(self, A, X) -> torch.Tensor:\n",
    "        # your code here\n",
    "        # Perform graph convolution: Z = ÂXW       \n",
    "        X_hat = torch.spmm(A, X)\n",
    "#         X_hat = torch.spmm(X, A)\n",
    "        Z = torch.spmm(X_hat, self.W)\n",
    "\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "813e9d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_feature = X.shape[1]\n",
    "# W = Parameter(torch.Tensor(in_feature, 48))\n",
    "# torch.nn.init.kaiming_uniform_(W)  # Xavier/Glorot initi\n",
    "\n",
    "# A_hat_torch = get_A_hat_torch(A)\n",
    "# X_hat = torch.spmm(A_hat_torch, X)\n",
    "# # X_hat = torch.spmm(X, A_hat)\n",
    "# Z = torch.spmm(X_hat, W)\n",
    "\n",
    "# A_hat_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb94fa1",
   "metadata": {},
   "source": [
    "Run the following cell to initialize and test your implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07028e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1822,  0.1992,  0.0685,  ..., -0.5589,  1.0460, -0.5471],\n",
       "        [ 1.5105,  0.0549,  0.3791,  ...,  0.1812, -1.7140, -0.5104],\n",
       "        [ 0.2074, -1.2355, -2.3261,  ..., -1.6745, -1.0768,  0.3438],\n",
       "        ...,\n",
       "        [ 1.3739, -0.9813,  0.9853,  ..., -0.4488, -0.1333,  0.5263],\n",
       "        [ 1.1874, -1.6371, -1.0612,  ..., -0.5831, -0.0058,  0.2217],\n",
       "        [ 0.8293, -0.3370,  0.5844,  ...,  0.3284, -0.0392, -1.2089]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features = X.shape[1]\n",
    "out_features = 48\n",
    "A_hat_torch = get_A_hat_torch(A)\n",
    "\n",
    "conv = GraphConvolutionLayer(in_features, out_features)\n",
    "conv(A_hat_torch, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae08cf12-16ae-408d-9713-3af4d72afa7e",
   "metadata": {},
   "source": [
    "### Task 3b: Implement the Graph Convolutional Model\n",
    "\n",
    "Implement the GCN as specified in the paper [1]. Concretely, implement a two-layer GCN with a ReLU activation function and dropout after the first layer, and with a softmax layer after the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e27f2a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import diags\n",
    "from torch.nn import Parameter\n",
    "def get_A_hat_torch(A):\n",
    "    A_tilde = coo_matrix(A, dtype=float)\n",
    "    degrees = A_tilde.sum(axis=1).flatten().A\n",
    "    Diag_matrix = diags(degrees, list(range(len(degrees))), dtype=float)\n",
    "    A_hat = (Diag_matrix.power(-0.5) @ A_tilde @ Diag_matrix.power(-0.5)).tocoo()\n",
    "\n",
    "\n",
    "    # A as sparse PyTorch tensor\n",
    "    indices = np.vstack((A_hat.row, A_hat.col))\n",
    "    A_hat_torch = torch.sparse_coo_tensor(indices, A_hat.data, dtype=torch.float)\n",
    "    return A_hat_torch\n",
    "\n",
    "\n",
    "class GraphConvolutionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Graph Convolution Layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # your code here\n",
    "        # Define learnable weight matrix\n",
    "        self.W = Parameter(torch.Tensor(in_features, out_features))\n",
    "        torch.nn.init.kaiming_uniform_(self.W)  # Xavier/Glorot initialization\n",
    "        \n",
    "    def forward(self, A, X) -> torch.Tensor:\n",
    "        # your code here\n",
    "        # Perform graph convolution: Z = ÂXW       \n",
    "#         X_hat = torch.spmm(A, X)\n",
    "        X_hat = torch.spmm(X, A)\n",
    "        Z = torch.spmm(X_hat, self.W)\n",
    "\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5bb694b-1728-4696-9b8e-eb93c5f2fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_size, out_features):\n",
    "        super().__init__()\n",
    "        # your code here\n",
    "        self.layer1 = GraphConvolutionLayer(in_features, hidden_size)\n",
    "        self.layer2 = GraphConvolutionLayer(hidden_size, out_features)\n",
    "\n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "        # your code here\n",
    "        X_mid = F.relu(self.layer1(X, A))\n",
    "        result = F.softmax(self.layer2(X_mid, A))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640ccfc8-6f4a-4fc9-911f-0635e3e5e193",
   "metadata": {},
   "source": [
    "Run the following cell to initialize and test your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6abd3180-b720-4b5d-9d19-dbdfa6953c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yk/tq09n1b16d3gv0_xwdh9yw600000gn/T/ipykernel_22209/849392476.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = F.softmax(self.layer2(X_mid, A))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4.0806e-02, 1.5808e-01, 3.2214e-03,  ..., 7.6862e-01, 4.8806e-03,\n",
       "         2.0993e-02],\n",
       "        [3.4422e-02, 9.8172e-03, 2.2864e-02,  ..., 7.5938e-01, 1.5408e-02,\n",
       "         1.7496e-02],\n",
       "        [1.2730e-01, 5.9436e-03, 4.9778e-01,  ..., 2.8078e-01, 1.7351e-02,\n",
       "         5.9357e-02],\n",
       "        ...,\n",
       "        [5.9801e-04, 3.2077e-01, 2.5444e-04,  ..., 3.5504e-03, 7.1108e-04,\n",
       "         6.6268e-01],\n",
       "        [3.3892e-02, 1.1778e-02, 2.8148e-02,  ..., 8.4863e-01, 9.6374e-03,\n",
       "         6.0665e-02],\n",
       "        [3.9481e-02, 6.6859e-03, 7.6810e-02,  ..., 4.1324e-01, 8.6512e-03,\n",
       "         3.5710e-01]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features = X.shape[1]\n",
    "hidden_size = 48\n",
    "out_features = num_labels.size\n",
    "\n",
    "model = GCN(in_features, hidden_size, out_features)\n",
    "\n",
    "y_pred = model(X, get_A_hat_torch(A))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a76d1a",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "\n",
    "In normal circumstances the GCN updates its internal representation for all nodes in the graph after each pass. In other words, the GCN operates on the entire graph at once, rather than on just the training, test, or validation set. Since these sets are disjoint, it necessarily means that only part of the class labels are available each time. This is called *semi-supervised learning*. Because the model sees the entire graph each pass, it still outputs predictions for all the nodes. However, by just calculating the loss and accuracy on a specific split, we ensure that only the error on the nodes in that split is backpropagated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16fa87d",
   "metadata": {},
   "source": [
    "### Task 4: Implementing evaluation metrics\n",
    "\n",
    "Write a procedure to calculate the loss *and* a procedure to calculate the accuracy. Assume that we have a tensor with true labels, `y_true`, and a tensor with predicted labels, `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0922ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true) -> float:\n",
    "    # your code here\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1)\n",
    "    \n",
    "    # Calculate the number of values that are the same in corresponding positions\n",
    "    matching_values = sum(y_hat == y for y_hat, y in zip(y_pred_labels, y_true))\n",
    "    accuracy = matching_values/len(y_pred_labels)\n",
    "    \n",
    "    return accuracy\n",
    "    \n",
    "\n",
    "def compute_loss(y_pred, y_true) -> torch.Tensor:\n",
    "    # your code here\n",
    "    m = nn.LogSoftmax(dim=1)\n",
    "    loss = loss_function(m(y_pred), (y_true))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7829a3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 7])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a7a5e7-cdf3-4960-af2f-9a96d9a9fc0a",
   "metadata": {},
   "source": [
    "Run the following cell to test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1e2dbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: tensor([4, 4, 2, 6, 1, 3, 1, 4, 4, 4])\n",
      "True labels: [5, 0, 3, 3, 4, 4, 1, 5, 5, 1]\n",
      "Accuracy: 0.112\n",
      "Loss: 2.004\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "y_pred_labels = torch.argmax(y_pred, dim=1)\n",
    "print(f'Predicted labels: {y_pred_labels[:10]}')\n",
    "print(f'True labels: {y_true[:10]}')\n",
    "\n",
    "acc = compute_accuracy(y_pred, torch.tensor(y_true))\n",
    "print(f'Accuracy: {acc:.3f}')\n",
    "\n",
    "loss = compute_loss(y_pred, torch.tensor(y_true))\n",
    "print(f'Loss: {loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95f107-2f27-47f2-a827-96337ab6aeda",
   "metadata": {},
   "source": [
    "### Task 5a: Implement the training loop\n",
    "\n",
    "Write a procedure to train the model. Specifically, create a loop that passes the entire graph through the model every epoch, while computing the loss and accuracy on just the training set. Use the Adam optimizer and the negative log likelihood loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53423cec-086b-4110-a8b2-206e7f02c9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 - loss: 1.9501\tacc: 0.1679\n",
      "Epoch   2 - "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yk/tq09n1b16d3gv0_xwdh9yw600000gn/T/ipykernel_22209/849392476.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = F.softmax(self.layer2(X_mid, A))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.8345\tacc: 0.3274\n",
      "Epoch   3 - loss: 1.7502\tacc: 0.4215\n",
      "Epoch   4 - loss: 1.6715\tacc: 0.5175\n",
      "Epoch   5 - loss: 1.6014\tacc: 0.5879\n",
      "Epoch   6 - loss: 1.5460\tacc: 0.6529\n",
      "Epoch   7 - loss: 1.5047\tacc: 0.6967\n",
      "Epoch   8 - loss: 1.4687\tacc: 0.7336\n",
      "Epoch   9 - loss: 1.4361\tacc: 0.7597\n",
      "Epoch  10 - loss: 1.4081\tacc: 0.7799\n",
      "Epoch  11 - loss: 1.3842\tacc: 0.8001\n",
      "Epoch  12 - loss: 1.3623\tacc: 0.8203\n",
      "Epoch  13 - loss: 1.3432\tacc: 0.8395\n",
      "Epoch  14 - loss: 1.3282\tacc: 0.8513\n",
      "Epoch  15 - loss: 1.3159\tacc: 0.8661\n",
      "Epoch  16 - loss: 1.3051\tacc: 0.8720\n",
      "Epoch  17 - loss: 1.2954\tacc: 0.8823\n",
      "Epoch  18 - loss: 1.2868\tacc: 0.8902\n",
      "Epoch  19 - loss: 1.2797\tacc: 0.8941\n",
      "Epoch  20 - loss: 1.2732\tacc: 0.9000\n",
      "Epoch  21 - loss: 1.2672\tacc: 0.9065\n",
      "Epoch  22 - loss: 1.2620\tacc: 0.9114\n",
      "Epoch  23 - loss: 1.2578\tacc: 0.9138\n",
      "Epoch  24 - loss: 1.2542\tacc: 0.9183\n",
      "Epoch  25 - loss: 1.2508\tacc: 0.9207\n",
      "Epoch  26 - loss: 1.2476\tacc: 0.9242\n",
      "Epoch  27 - loss: 1.2448\tacc: 0.9261\n",
      "Epoch  28 - loss: 1.2422\tacc: 0.9286\n",
      "Epoch  29 - loss: 1.2399\tacc: 0.9311\n",
      "Epoch  30 - loss: 1.2379\tacc: 0.9345\n",
      "Epoch  31 - loss: 1.2359\tacc: 0.9365\n",
      "Epoch  32 - loss: 1.2340\tacc: 0.9370\n",
      "Epoch  33 - loss: 1.2322\tacc: 0.9404\n",
      "Epoch  34 - loss: 1.2307\tacc: 0.9404\n",
      "Epoch  35 - loss: 1.2296\tacc: 0.9414\n",
      "Epoch  36 - loss: 1.2287\tacc: 0.9424\n",
      "Epoch  37 - loss: 1.2281\tacc: 0.9424\n",
      "Epoch  38 - loss: 1.2275\tacc: 0.9429\n",
      "Epoch  39 - loss: 1.2269\tacc: 0.9429\n",
      "Epoch  40 - loss: 1.2263\tacc: 0.9434\n",
      "Epoch  41 - loss: 1.2258\tacc: 0.9434\n",
      "Epoch  42 - loss: 1.2253\tacc: 0.9434\n",
      "Epoch  43 - loss: 1.2248\tacc: 0.9439\n",
      "Epoch  44 - loss: 1.2242\tacc: 0.9444\n",
      "Epoch  45 - loss: 1.2236\tacc: 0.9444\n",
      "Epoch  46 - loss: 1.2231\tacc: 0.9458\n",
      "Epoch  47 - loss: 1.2226\tacc: 0.9463\n",
      "Epoch  48 - loss: 1.2220\tacc: 0.9473\n",
      "Epoch  49 - loss: 1.2213\tacc: 0.9473\n",
      "Epoch  50 - loss: 1.2207\tacc: 0.9478\n",
      "Epoch  51 - loss: 1.2202\tacc: 0.9483\n",
      "Epoch  52 - loss: 1.2198\tacc: 0.9483\n",
      "Epoch  53 - loss: 1.2194\tacc: 0.9483\n",
      "Epoch  54 - loss: 1.2192\tacc: 0.9483\n",
      "Epoch  55 - loss: 1.2189\tacc: 0.9483\n",
      "Epoch  56 - loss: 1.2186\tacc: 0.9483\n",
      "Epoch  57 - loss: 1.2182\tacc: 0.9493\n",
      "Epoch  58 - loss: 1.2178\tacc: 0.9493\n",
      "Epoch  59 - loss: 1.2175\tacc: 0.9493\n",
      "Epoch  60 - loss: 1.2173\tacc: 0.9493\n",
      "Epoch  61 - loss: 1.2171\tacc: 0.9493\n",
      "Epoch  62 - loss: 1.2169\tacc: 0.9493\n",
      "Epoch  63 - loss: 1.2166\tacc: 0.9493\n",
      "Epoch  64 - loss: 1.2163\tacc: 0.9503\n",
      "Epoch  65 - loss: 1.2159\tacc: 0.9513\n",
      "Epoch  66 - loss: 1.2156\tacc: 0.9522\n",
      "Epoch  67 - loss: 1.2154\tacc: 0.9517\n",
      "Epoch  68 - loss: 1.2152\tacc: 0.9522\n",
      "Epoch  69 - loss: 1.2150\tacc: 0.9522\n",
      "Epoch  70 - loss: 1.2148\tacc: 0.9522\n",
      "Epoch  71 - loss: 1.2146\tacc: 0.9527\n",
      "Epoch  72 - loss: 1.2143\tacc: 0.9532\n",
      "Epoch  73 - loss: 1.2141\tacc: 0.9532\n",
      "Epoch  74 - loss: 1.2139\tacc: 0.9532\n",
      "Epoch  75 - loss: 1.2138\tacc: 0.9527\n",
      "Epoch  76 - loss: 1.2136\tacc: 0.9527\n",
      "Epoch  77 - loss: 1.2135\tacc: 0.9532\n",
      "Epoch  78 - loss: 1.2134\tacc: 0.9532\n",
      "Epoch  79 - loss: 1.2132\tacc: 0.9532\n",
      "Epoch  80 - loss: 1.2131\tacc: 0.9532\n",
      "Epoch  81 - loss: 1.2130\tacc: 0.9532\n",
      "Epoch  82 - loss: 1.2129\tacc: 0.9537\n",
      "Epoch  83 - loss: 1.2127\tacc: 0.9537\n",
      "Epoch  84 - loss: 1.2124\tacc: 0.9537\n",
      "Epoch  85 - loss: 1.2118\tacc: 0.9547\n",
      "Epoch  86 - loss: 1.2113\tacc: 0.9547\n",
      "Epoch  87 - loss: 1.2109\tacc: 0.9552\n",
      "Epoch  88 - loss: 1.2107\tacc: 0.9557\n",
      "Epoch  89 - loss: 1.2106\tacc: 0.9557\n",
      "Epoch  90 - loss: 1.2105\tacc: 0.9557\n",
      "Epoch  91 - loss: 1.2105\tacc: 0.9557\n",
      "Epoch  92 - loss: 1.2104\tacc: 0.9557\n",
      "Epoch  93 - loss: 1.2104\tacc: 0.9557\n",
      "Epoch  94 - loss: 1.2103\tacc: 0.9557\n",
      "Epoch  95 - loss: 1.2103\tacc: 0.9557\n",
      "Epoch  96 - loss: 1.2102\tacc: 0.9557\n",
      "Epoch  97 - loss: 1.2100\tacc: 0.9557\n",
      "Epoch  98 - loss: 1.2098\tacc: 0.9562\n",
      "Epoch  99 - loss: 1.2097\tacc: 0.9562\n",
      "Epoch 100 - loss: 1.2096\tacc: 0.9567\n",
      "Epoch 101 - loss: 1.2095\tacc: 0.9567\n",
      "Epoch 102 - loss: 1.2094\tacc: 0.9567\n",
      "Epoch 103 - loss: 1.2093\tacc: 0.9567\n",
      "Epoch 104 - loss: 1.2092\tacc: 0.9567\n",
      "Epoch 105 - loss: 1.2092\tacc: 0.9567\n",
      "Epoch 106 - loss: 1.2092\tacc: 0.9567\n",
      "Epoch 107 - loss: 1.2092\tacc: 0.9567\n",
      "Epoch 108 - loss: 1.2091\tacc: 0.9567\n",
      "Epoch 109 - loss: 1.2091\tacc: 0.9567\n",
      "Epoch 110 - loss: 1.2091\tacc: 0.9567\n",
      "Epoch 111 - loss: 1.2090\tacc: 0.9567\n",
      "Epoch 112 - loss: 1.2090\tacc: 0.9567\n",
      "Epoch 113 - loss: 1.2090\tacc: 0.9567\n",
      "Epoch 114 - loss: 1.2090\tacc: 0.9567\n",
      "Epoch 115 - loss: 1.2090\tacc: 0.9567\n",
      "Epoch 116 - loss: 1.2090\tacc: 0.9567\n",
      "Epoch 117 - loss: 1.2089\tacc: 0.9567\n",
      "Epoch 118 - loss: 1.2089\tacc: 0.9567\n",
      "Epoch 119 - loss: 1.2089\tacc: 0.9567\n",
      "Epoch 120 - loss: 1.2089\tacc: 0.9567\n",
      "Epoch 121 - loss: 1.2089\tacc: 0.9567\n",
      "Epoch 122 - loss: 1.2089\tacc: 0.9567\n",
      "Epoch 123 - loss: 1.2089\tacc: 0.9567\n",
      "Epoch 124 - loss: 1.2089\tacc: 0.9567\n",
      "Epoch 125 - loss: 1.2088\tacc: 0.9567\n",
      "Epoch 126 - loss: 1.2088\tacc: 0.9567\n",
      "Epoch 127 - loss: 1.2088\tacc: 0.9567\n",
      "Epoch 128 - loss: 1.2088\tacc: 0.9567\n",
      "Epoch 129 - loss: 1.2088\tacc: 0.9567\n",
      "Epoch 130 - loss: 1.2088\tacc: 0.9567\n",
      "Epoch 131 - loss: 1.2088\tacc: 0.9567\n",
      "Epoch 132 - loss: 1.2088\tacc: 0.9567\n",
      "Epoch 133 - loss: 1.2087\tacc: 0.9567\n",
      "Epoch 134 - loss: 1.2087\tacc: 0.9567\n",
      "Epoch 135 - loss: 1.2087\tacc: 0.9567\n",
      "Epoch 136 - loss: 1.2087\tacc: 0.9567\n",
      "Epoch 137 - loss: 1.2087\tacc: 0.9567\n",
      "Epoch 138 - loss: 1.2087\tacc: 0.9567\n",
      "Epoch 139 - loss: 1.2087\tacc: 0.9567\n",
      "Epoch 140 - loss: 1.2087\tacc: 0.9567\n",
      "Epoch 141 - loss: 1.2087\tacc: 0.9567\n",
      "Epoch 142 - loss: 1.2087\tacc: 0.9567\n",
      "Epoch 143 - loss: 1.2087\tacc: 0.9567\n",
      "Epoch 144 - loss: 1.2087\tacc: 0.9567\n",
      "Epoch 145 - loss: 1.2086\tacc: 0.9567\n",
      "Epoch 146 - loss: 1.2086\tacc: 0.9567\n",
      "Epoch 147 - loss: 1.2086\tacc: 0.9567\n",
      "Epoch 148 - loss: 1.2085\tacc: 0.9567\n",
      "Epoch 149 - loss: 1.2082\tacc: 0.9572\n",
      "Epoch 150 - loss: 1.2081\tacc: 0.9572\n",
      "Epoch 151 - loss: 1.2081\tacc: 0.9577\n",
      "Epoch 152 - loss: 1.2080\tacc: 0.9577\n",
      "Epoch 153 - loss: 1.2078\tacc: 0.9577\n",
      "Epoch 154 - loss: 1.2077\tacc: 0.9577\n",
      "Epoch 155 - loss: 1.2078\tacc: 0.9577\n",
      "Epoch 156 - loss: 1.2078\tacc: 0.9577\n",
      "Epoch 157 - loss: 1.2077\tacc: 0.9577\n",
      "Epoch 158 - loss: 1.2077\tacc: 0.9577\n",
      "Epoch 159 - loss: 1.2077\tacc: 0.9577\n",
      "Epoch 160 - loss: 1.2077\tacc: 0.9577\n",
      "Epoch 161 - loss: 1.2077\tacc: 0.9577\n",
      "Epoch 162 - loss: 1.2077\tacc: 0.9577\n",
      "Epoch 163 - loss: 1.2077\tacc: 0.9577\n",
      "Epoch 164 - loss: 1.2077\tacc: 0.9577\n",
      "Epoch 165 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 166 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 167 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 168 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 169 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 170 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 171 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 172 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 173 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 174 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 175 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 176 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 177 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 178 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 179 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 180 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 181 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 182 - loss: 1.2076\tacc: 0.9577\n",
      "Epoch 183 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 184 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 185 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 186 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 187 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 188 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 189 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 190 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 191 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 192 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 193 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 194 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 195 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 196 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 197 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 198 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 199 - loss: 1.2075\tacc: 0.9577\n",
      "Epoch 200 - loss: 1.2075\tacc: 0.9577\n"
     ]
    }
   ],
   "source": [
    "# set hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_epoch = 200\n",
    "in_features = X.shape[1]\n",
    "hidden_size = 48\n",
    "out_features = num_labels.size\n",
    "\n",
    "model = GCN(in_features, hidden_size, out_features)\n",
    "loss_function = nn.NLLLoss()\n",
    "A_hat_torch = get_A_hat_torch(A)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr = learning_rate)\n",
    "\n",
    "for epoch in range(1, num_epoch+1):\n",
    "    print(f'Epoch {epoch:3d} - ', end='')\n",
    "\n",
    "    # allow model parameters to be learned   \n",
    "    model.train()  \n",
    "\n",
    "    # your code here\n",
    "    pred = model(X, A_hat_torch)\n",
    "#     loss = compute_loss(pred, y_true)\n",
    "    loss = compute_loss(pred[torch.nonzero(torch.tensor(train_idx)).squeeze()], torch.tensor(y_true)[torch.nonzero(torch.tensor(train_idx)).squeeze()])\n",
    "    \n",
    "    acc = compute_accuracy(pred[torch.nonzero(torch.tensor(train_idx)).squeeze()], torch.tensor(y_true)[torch.nonzero(torch.tensor(train_idx)).squeeze()])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss = float(loss)  # release memory of computation graph\n",
    "\n",
    "    print(f'loss: {loss:0.4f}\\tacc: {acc:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc2246-8bd8-4d5b-aca1-89d2a0a65211",
   "metadata": {},
   "source": [
    "### Task 5b: Implement the test procedure\n",
    "\n",
    "Write a procedure to test the now-trained model. Ensure that the weights of your model are frozen during testing, and that the loss and accuracy scores are calculated on just the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4156df0f-3662-471c-90e8-1eb30d9d24e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 1.3308\n",
      "test acc: 0.8346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yk/tq09n1b16d3gv0_xwdh9yw600000gn/T/ipykernel_22209/849392476.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = F.softmax(self.layer2(X_mid, A))\n"
     ]
    }
   ],
   "source": [
    "# freeze model parameters for evaluation\n",
    "model.eval()\n",
    "\n",
    "# your code here\n",
    "pred = model(X, A_hat_torch)\n",
    "loss = compute_loss(pred[torch.nonzero(torch.tensor(test_idx)).squeeze()], torch.tensor(y_true)[torch.nonzero(torch.tensor(test_idx)).squeeze()])\n",
    "acc = compute_accuracy(pred[torch.nonzero(torch.tensor(test_idx)).squeeze()], torch.tensor(y_true)[torch.nonzero(torch.tensor(test_idx)).squeeze()])\n",
    "\n",
    "loss = float(loss)  # release memory of computation graph\n",
    "\n",
    "print(f'test loss: {loss:0.4f}\\ntest acc: {acc:0.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
